What is the difference between linear SVMs and logistic regression?

1)https://www.quora.com/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression/answer/Prasoon-Goyal?srid=ti32

Machine Learning vides which are closer to Bishop.
1)http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/lectures.html
2)http://videolectures.net/mlss09uk_bishop_ibi/?q=bishop


Beta distribution:
https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/class-slides/MIT18_05S14_class14_slides.pdf

Latent Dirchilet Allocation:
1)https://www.youtube.com/watch?v=DWJYZq_fQ2A - very good introduction

Difference between Maximum likelihood, maximum aposteriori, Bayesian estimation.
In ML and MAP we get a single estimation of the parameter. Though in MAP method we use prior information of the parameter, we still get a single estimate. We do not worry about the posterior distribution. Where as in the Bayesian estimation, we consider the posterior distribution which uses the idea called conjugate priors - the prior and the posterior distibutions will be same. The prior distribution parameters will be updated in the posterior distribution. 

1)https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf


Hidden Markov model:
a)Unsupervised learning
1)read about viterbi algorithm and farword backword algorithm from the NPTEL pushpak bhattacharya video lectures
2)Training: http://cs.au.dk/~cstorm/courses/MLiB_f14/slides/hidden-markov-models-4.pdf
a)Train parameters for single instance : https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
b)How To train parameters for multiple instances (X's) and multiple state sequence - last but one slide
c)How To train parameters for multiple instances (X's) - last slide 
d)http://users-cs.au.dk/cstorm/courses/PRiB_f12/slides/ (Bishop book  related slides )
b)Supervised learning
1)View Anoop Sarkar youtube channel

Principal component analysis:
To know why we have to find eigen vectors use Bishop text book pg number 562


Singular Valued decomposition:
http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm
https://datajobs.com/data-science-repo/SVD-Tutorial-%5bKirk-Baker%5d.pdf (Page 21)


Expectation Maximization:
1)Coursera advance machine learning specalization
2)https://people.inf.ethz.ch/ganeao/em_tutorial.pdf
3)Differences between expetation maximization and variational bayes expectation maximization - https://jhaberstro.blogspot.in/2017/09/expectation-maximization-vs-variational.html


Variational Inference:
Refer wikipedia - https://en.wikipedia.org/wiki/Variational_Bayesian_methods
Video lecture - http://www.fields.utoronto.ca/video-archive/2015/02/323-3554

Latent Semantic Analysis:
https://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class11.pdf


Advance Machine Learning:

1)Coursera course
2)https://people.cs.pitt.edu/~milos/courses/cs3750/


Back Propagation through time:
https://kharshit.github.io/blog/2019/02/22/backpropagation-through-time 
http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf
https://towardsdatascience.com/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956fbea32704


Language Models:

https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#nmt-recap
